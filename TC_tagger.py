import os
import sys
sys.path.append(os.path.dirname(__file__))
from konlpy_tc.tag import Okt_edit
import emoji
import re
import pandas as pd



class tagger():

    def __init__(self):
        self.emoji_dic = emoji.UNICODE_EMOJI
        self.emoji_list = map(lambda x: ''.join(x.split()), emoji.UNICODE_EMOJI.keys())
        self.re_emoji = re.compile('|'.join(re.escape(p) for p in self.emoji_list))
        self.okt_edit = Okt_edit()
        self.re_hashtag = re.compile('')
        
       
    def emoticon(self, result):
        emo_lst = []
        for idx, (token, _) in enumerate(result):
            if re.search(self.re_emoji, token):
                lst = [(x+'_'+self.emoji_dic[x][1:-1], 'Emoji') for x in token]
                emo_lst.append((idx, lst))
        emo_lst.reverse()
        for idx, emo in emo_lst:
            result.pop(idx)
            result[idx:idx] = emo
        return result
    
    def hashtag(self, result):
        for idx, (token, tag) in enumerate(result):
            if tag == 'Hashtag':
                token = re.search('[#](\w+)', token).group(1)
                phrase_lst = self.okt_edit.phrases(token)
                if len(phrase_lst) == 0:
                    phrase = token
                else:
                    phrase = phrase_lst[0]
                new_token = re.sub(phrase, ' '+phrase+' ' , token).strip().split()
                h = []
                for x in new_token:
                    if x == phrase:
                        h.append((x, 'Hashtag_Noun'))
                    else:
                        tmp = self.okt_edit.pos(x)
                        for token_, tag_ in tmp:
                            h.append((token_, 'Hashtag_'+tag))         
                result[idx] = tuple(h)
        return result

    def tag(self, text):
        result = self.okt_edit.pos(text)
        result = self.emoticon(result)
        result = self.hashtag(result)
        return result

    def tokenizer(self, text):
        tag_result = self.tag(text)
        token_lst = []
        for x in tag_result:
            if type(x[0]) == str:
                token_lst.append(x[0])
            else:
                for y in x:
                    token_lst.append(y[0])
        return token_lst

    def nouns(self, text):
        tag_result = self.tag(text)
        nouns_lst = []
        for x in tag_result:
            if type(x[0]) == str and x[1] == 'Noun':
                nouns_lst.append(x[0])
            elif type(x[0]) == str:
                pass
            else:
                for y in x:
                    if y[1] == 'Hashtag_Noun':
                        nouns_lst.append(y[0])
        return nouns_lst

if __name__ == "__main__":
    text1 = 'ë‹¤ì´ì–´íŠ¸ í•´ì•¼ë˜ëŠ”ë°...ğŸ˜‚ #ë©‹ì§íœ˜íŠ¸ë‹ˆìŠ¤ì—°ì‚°ì  #ì—°ì‚°ë™pt'
    text2 = 'ëŸ½ìŠ¤íƒ€ ê·¸ìì²´â¤â¤\n#ëŸ½ìŠ¤íƒ€ê·¸ë¨ #ìš´ë™í•˜ëŠ”ì»¤í”Œ #íƒœë‹'
    text3 = 'á„‹á…®á„…á…°á„€á…µ á„€á…¡á†¸á„Œá…¡á„€á…µ \U0001fa78ğŸ’© á„Šá…¡á„€á…© ğŸ¤® á„’á…¡á„€á…© á„‹á…«á„€á…³á„…á…¢.. ì§€ë°œë¡œ ì¼„ë„¬ë“¤ì–´ê°€ì„œ ëª¸ ë§ê³  ìê³ ìˆê³ . ã…  á„‹á…¥á†·á„†á…¡ á„‰á…®á„‹á…¥á†¸ á„€á…¡á†« á„‰á…¡á„‹á…µ á„€á…³ á„Œá…©á‡‚á„‹á…³á†« á„‹á…§á†¯á„‡á…µá†¼á„‹á…¥á„‘á…©á„ƒá…© á„á…¥á„á…µá„á…²á„ƒá…© á„€á…¥á†«á„ƒá…³á„…á…µá„Œá…µá„ƒá…© á„‹á…¡á†­á„€á…© ã… . á„€á…£á†¯á„€á…®á†¨ á„‡á…§á†¼á„‹á…¯á†«á„‹á…ªá„‰á…¥ á„’á…§á†¯á„‹á…¢á†¨á„€á…¥á†·á„‰á…¡á„Œá…²á†¼. á„‹á…¡á„‘á…³á„Œá…µá„†á…¡ á„‚á…¢á„‰á…¢á„á…®. My boy doesnâ€™t feel well today and finally paid a visit to a local vet for some \U0001fa78 tests done. ğŸ˜­ğŸ˜­-#billie #puppy #puppystagram #puppylove #puppyson #maltipoo #dog #dogstagram #puppymomlife #mydogismychild #daily #2020 #á„‡á…µá†¯á„…á…µ #á„€á…¢á„…á…µá†«á„‹á…µ #á„†á…¡á†¯á„á…µá„‘á…® #á„€á…¢á„‹á…¡á„ƒá…³á†¯ #á„€á…¢á„‰á…³á„á…¡á„€á…³á„…á…¢á†· #á„€á…¡á†¼á„‹á…¡á„Œá…µ #á„€á…¡á†¼á„‹á…¡á„Œá…µá„‰á…³á„á…¡á„€á…³á„…á…¢á†· #á„ƒá…¢á†¼á„ƒá…¢á†¼á„‹á…µ #á„†á…¥á†¼á„†á…¥á†¼á„‹á…µ #á„†á…¥á†¼á„†á…®á†¼á„‹á…µ #á„ˆá…©á„‰á…µá„…á…¢á„€á…µ #á„€á…¢á„Œá…µá†¸á„‰á…¡ #á„‹á…µá†¯á„‰á…¡á†¼ #á„€á…¢á†¼á„‹á…¥á†¯á„Œá…± #á„‚á…¢á„‰á…¢á„á…®'
    tc_tagger = tagger()
    print(tc_tagger.nouns(text3))
    # for t in [text1, text2, text3]:
    #     print('='*100)
    #     print('\nThreecow : ', tc_tagger.tag(t))
    #     print('\n', '='*100)
    #     print('\ntokenize ê²°ê³¼: ')
    #     print(tc_tagger.tokenizer(t))
    #     print('\nnouns ì¶”ì¶œ ê²°ê³¼: ')
    #     print(tc_tagger.nouns(t))